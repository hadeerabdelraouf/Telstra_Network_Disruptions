{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import *\n",
    "import string\n",
    "from functools import reduce\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Reading data\n",
    "event_type = pd.read_csv('../Data/event_type.csv')\n",
    "log_feature = pd.read_csv('../Data/log_feature.csv')\n",
    "resource_type = pd.read_csv('../Data/resource_type.csv')\n",
    "severity_type = pd.read_csv('../Data/severity_type.csv')\n",
    "test = pd.read_csv('../Data/test.csv')\n",
    "train = pd.read_csv('../Data/train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##event type denormalized\n",
    "event_type_flattened = pd.get_dummies(event_type, columns=[\"event_type\"])\n",
    "event_type_flattened = event_type_flattened.groupby(['id']).agg(['sum'])\n",
    "event_type_flattened.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Log featured denormalized\n",
    "log_feature_flatened = log_feature.pivot(index='id', columns='log_feature', values='volume')\n",
    "log_feature_flatened.reset_index(inplace = True)\n",
    "log_feature_flatened.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Resource type denormalized\n",
    "Resource_type_flattened = pd.get_dummies(resource_type, columns=[\"resource_type\"])\n",
    "Resource_type_flattened = Resource_type_flattened.groupby(['id']).agg(['sum'])\n",
    "Resource_type_flattened.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ideas for Features extraction\n",
    "* Number of events/feature/resource occured\n",
    "* Most used events/feature/resource occured --- Not applicable no duplication\n",
    "* Volume of feature / max value of feature\n",
    "* sum of total volume\n",
    "* Max events/feature/resource\n",
    "* Severity number\n",
    "* location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Number of events/feature/resource occured\n",
    "Event_count_feature = pd.DataFrame(event_type.groupby(['id']).size()).reset_index()\n",
    "Feature_count_feature = pd.DataFrame(log_feature.groupby(['id']).size()).reset_index()\n",
    "Resource_count_feature = pd.DataFrame(resource_type.groupby(['id']).size()).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Volume of feature / max value of feature\n",
    "max_volume_feature = pd.DataFrame(log_feature.groupby(['log_feature'])['volume'].agg(['max'])).reset_index()\n",
    "\n",
    "log_feature2 = log_feature.merge(max_volume_feature,how = 'left' , on = 'log_feature')\n",
    "log_feature2['volume_ratio'] = log_feature2['volume'] / log_feature2['max']\n",
    "\n",
    "log_feature2_flatened = log_feature2.pivot(index='id', columns='log_feature', values='volume_ratio')\n",
    "log_feature2_flatened.reset_index(inplace = True)\n",
    "log_feature2_flatened.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sum of total volume\n",
    "volume = pd.DataFrame(log_feature.groupby(['id'])['volume'].agg(['sum'])).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Max events/feature/resource\n",
    "event_type['event_number'] = event_type['event_type'].str.slice(-2)\n",
    "event_type['event_number'] = event_type['event_number'].astype(np.int64)\n",
    "max_event_number = pd.DataFrame(event_type.groupby(['id'])['event_number'].agg(['max'])).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Severity number\n",
    "severity_type['severity_type_number'] = severity_type['severity_type'].str.slice(-1)\n",
    "severity_type['severity_type_number'] = severity_type['severity_type_number'].astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## location\n",
    "train['location_number'] = train['location'].str.slice(-3)\n",
    "train['location_number'] = train['location_number'].apply(lambda x: x.lstrip('n'))\n",
    "train['location_number'] = train['location_number'].astype(np.int64)\n",
    "\n",
    "test['location_number'] = test['location'].str.slice(-3)\n",
    "test['location_number'] = test['location_number'].apply(lambda x: x.lstrip('n'))\n",
    "test['location_number'] = test['location_number'].astype(np.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADS Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## insert all dataframes to be merged in a list\n",
    "dfs = [train, event_type_flattened, log_feature_flatened, Resource_type_flattened , Event_count_feature , Feature_count_feature , Resource_count_feature\n",
    "      , log_feature2_flatened , volume , max_event_number , severity_type  ]\n",
    "\n",
    "dfs_test = [test, event_type_flattened, log_feature_flatened, Resource_type_flattened , Event_count_feature , Feature_count_feature , Resource_count_feature\n",
    "      , log_feature2_flatened , volume , max_event_number , severity_type  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## merge all variables dataframes together for train and test\n",
    "df_final = reduce(lambda left,right: pd.merge(left,right,on='id'), dfs)\n",
    "df_final_test = reduce(lambda left,right: pd.merge(left,right,on='id'), dfs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## remove categorical variables\n",
    "df_final.select_dtypes(include='object')\n",
    "df_final.drop(['location', 'severity_type'], axis=1 , inplace = True)\n",
    "df_final_test.drop(['location', 'severity_type'], axis=1 , inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideas:\n",
    "    * Remove constants --- done\n",
    "    * View correlation  --- impossible with high dimesnion\n",
    "    * Scatter plot matrix  -- impossible with high dimension\n",
    "    * Normalize variables   -- done\n",
    "    * PCA                  -- done\n",
    "    * Check variables statisticlly using Anova"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove constants\n",
    "df_describtion = df_final.describe().T\n",
    "df_describtion['max_min'] = df_describtion['max'] - df_describtion['min']\n",
    "constants_col_remove = list(df_describtion[df_describtion['max_min'] == 0].T.columns)\n",
    "df_final.drop(constants_col_remove, axis=1 , inplace = True)\n",
    "df_final_test.drop(constants_col_remove, axis=1 , inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Scatter plot matrix\n",
    "## it won't work cause we have many dimensions\n",
    "#spm = pd.plotting.scatter_matrix(df_final.iloc[:,:50], alpha=0.2, figsize=(15, 15), diagonal='hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Correlation\n",
    "#np.corrcoef(df_final.loc[:, df_final.columns != 'fault_severity'], df_final['fault_severity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corr = df_final.corr()\n",
    "# fig = plt.figure(figsize=(50, 50))\n",
    "# ax = fig.add_subplot(111)\n",
    "# cax = ax.matshow(corr,cmap='coolwarm', vmin=-1, vmax=1)\n",
    "# fig.colorbar(cax)\n",
    "# ticks = np.arange(0,len(df_final.columns),1)\n",
    "# ax.set_xticks(ticks)\n",
    "# plt.xticks(rotation=90)\n",
    "# ax.set_yticks(ticks)\n",
    "# ax.set_xticklabels(df_final.columns)\n",
    "# ax.set_yticklabels(df_final.columns)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Normalize data\n",
    "##extrat x and y\n",
    "X = df_final.loc[:, df_final.columns != 'fault_severity']\n",
    "y = df_final['fault_severity']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "Scale_model = StandardScaler()\n",
    "X_train_scaled = pd.DataFrame(Scale_model.fit_transform(X_train))\n",
    "X_train_scaled.columns = X_train.columns\n",
    "\n",
    "X_test_scaled = pd.DataFrame(Scale_model.transform(X_test))\n",
    "X_test_scaled.columns = X_test.columns\n",
    "\n",
    "df_final_test_scaled = pd.DataFrame(Scale_model.transform(df_final_test))\n",
    "df_final_test_scaled.columns = df_final_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA_varianceSearch (model,X_train_scaled,y_train,X_test_scaled,y_test,random_state,scoring):\n",
    "    n_comp = [1,0.95,0.90,0.85]\n",
    "    pca_output = {}\n",
    "    pca_acc = {}\n",
    "\n",
    "    for i in n_comp:\n",
    "        pca = PCA(i,random_state = random_state)\n",
    "        principalComponents = pca.fit_transform(X_train_scaled)\n",
    "        principalDf = pd.DataFrame(data = principalComponents)\n",
    "        principalDf.rename(columns=lambda x: 'pca' + str(x), inplace=True)\n",
    "        #principalDf[y_name] = y_train\n",
    "        ###test\n",
    "        principalDf_test1 = pd.DataFrame(pca.transform(X_test_scaled))\n",
    "        principalDf_test1.rename(columns=lambda x: 'pca' + str(x), inplace=True)\n",
    "        \n",
    "#         if test_transform == True:\n",
    "#             principalDf_test2 = pd.DataFrame(pca.transform(df_final_test_scaled))\n",
    "#             principalDf_test2.rename(columns=lambda x: 'pca' + str(x), inplace=True)\n",
    "        \n",
    "        #train the model\n",
    "        classifier = model().fit(principalDf,y_train)\n",
    "        acc_score = classifier.score(principalDf_test1,y_test)\n",
    "        pca_output[i] = (pca,principalDf,principalDf_test1,acc_score)\n",
    "        pca_acc[i] = acc_score\n",
    "    \n",
    "    max_acc = max(pca_acc.values())\n",
    "    position = int(np.where(list(pca_acc.values()) == max_acc)[0])\n",
    "    Max_Name = list(pca_output.keys())[position]\n",
    "    best_pca = pca_output[Max_Name][0]\n",
    "    \n",
    "    \n",
    "    \n",
    "    return pca_output , pca_acc , best_pca\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Getting the best pca and running it on test data\n",
    "pca_output , pca_acc , best_pca = PCA_varianceSearch (GradientBoostingClassifier,X_train_scaled,y_train,X_test_scaled,y_test,0,'accuracy')\n",
    "principalDf_test2 = pd.DataFrame(best_pca.transform(df_final_test_scaled))\n",
    "principalDf_test2.rename(columns=lambda x: 'pca' + str(x), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varience_ratio_df = pd.DataFrame(best_pca.explained_variance_ratio_ )\n",
    "varience_ratio_df['column_name'] = pca_output[best_pca.n_components][1].columns\n",
    "varience_ratio_df.sort_values(0 , inplace = True , ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pca = pca_output[best_pca.n_components][1]\n",
    "X_test_pca = pca_output[best_pca.n_components][2]\n",
    "X_train_pca['fault_severity'] = y_train\n",
    "X_test_pca['fault_severity'] = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pca_output[0.85][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_scaled.shape , y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## scatter plot matrix after pca\n",
    "#spm = pd.plotting.scatter_matrix(principalDf, alpha=0.2, figsize=(50, 50), diagonal='hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store df_final\n",
    "%store df_final_test\n",
    "\n",
    "%store X_train_pca\n",
    "%store X_test_pca\n",
    "%store principalDf_test2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
