{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import *\n",
    "\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to assist the main classifier, classification phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Parameter_initialization():\n",
    "\n",
    "    param_xgb = {'n_estimators':[200],\n",
    "                   'min_samples_split':[302],\n",
    "                   'max_depth':[11],\n",
    "                   'min_samples_leaf':[1],\n",
    "                   'max_features':[30],\n",
    "                   'subsample':[0.8],\n",
    "                   'learning_rate':[0.1]\n",
    "                  }\n",
    "\n",
    "\n",
    "\n",
    "    param_RF  = {'criterion':['gini','entropy'],\n",
    "                  'n_estimators':[10,15,20,25,30],\n",
    "                  'min_samples_leaf':[1,2,3],\n",
    "                  'min_samples_split':[3,4,5,6,7], \n",
    "                  'random_state':[123]\n",
    "                  }\n",
    "    \n",
    "    param_svm  = {'C':[ 1],\n",
    "                      'gamma':[0.001],\n",
    "                      'kernel' : ['rbf']\n",
    "                      }\n",
    "\n",
    "    param_KNN  = {'n_neighbors':[5,6,7,8,9,10],\n",
    "                  'leaf_size':[1,2,3,5],\n",
    "                  'weights':['uniform', 'distance'],\n",
    "                  'algorithm':['auto', 'ball_tree','kd_tree','brute']\n",
    "                  }\n",
    "\n",
    "    param_nn  = { 'solver': ['sgd', 'adam','lbfgs'],\n",
    "              'alpha': 10.0 ** -np.arange(1, 7),\n",
    "              'hidden_layer_sizes':np.arange(5, 12),\n",
    "              'max_iter': [500,1000,1500],\n",
    "             'activation': ['tanh', 'relu']\n",
    "              }\n",
    "\n",
    "    Model_dict = {'XGB':(GradientBoostingClassifier,0,param_xgb),\n",
    "              'SVM':(SVC,0,param_svm),\n",
    "              'KNN' :(KNeighborsClassifier,0,param_KNN),\n",
    "              'RF':(RandomForestClassifier,0,param_RF),\n",
    "              'NN': (MLPClassifier,0,param_nn)\n",
    "              }\n",
    "    \n",
    "    return Model_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_values (classifier,X,Y,scoring):\n",
    "    ##make the scoring metric a function\n",
    "    if scoring in 'f1':\n",
    "        scoring = 'f1_score'\n",
    "    if scoring in 'neg_log_loss':\n",
    "        scoring = 'log_loss'\n",
    "    else:\n",
    "        scoring = scoring + '_score'\n",
    "    \n",
    "    dtest_predictions = classifier.predict(X)\n",
    "    dtest_predprob = classifier.predict_proba(X)\n",
    "    if scoring == 'log_loss':\n",
    "        dtest_score = eval(scoring)(y_true = Y,y_pred =dtest_predprob,labels = list(classifier.classes_))\n",
    "    else:\n",
    "        dtest_score = eval(scoring)(Y,dtest_predictions)\n",
    "    return dtest_predictions, dtest_predprob , dtest_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genral_purpose_gridsearch (model_name,alg,parameters,random_state,n_jobs,cv_folds,verbose,X_train,y_train,scoring,X_test,y_test):\n",
    "    if model_name == 'SVM':\n",
    "        alg = alg(probability=True,random_state=random_state)\n",
    "    else :\n",
    "        if model_name == 'KNN':\n",
    "            alg = alg()\n",
    "        else:\n",
    "            alg = alg(random_state=random_state)\n",
    "    gsearch1 = GridSearchCV(estimator = alg, \n",
    "    param_grid = parameters, scoring=scoring,n_jobs=n_jobs,iid=False, cv=cv_folds,verbose = verbose)\n",
    "\n",
    "    gsearch1.fit(X_train,y_train)\n",
    "    grid_test_predictions,grid_test_predictions_prob,test_accuracy_score = predict_values(gsearch1.best_estimator_,X_test,y_test,scoring)\n",
    "    return gsearch1 , gsearch1.best_estimator_ , gsearch1.best_params_ , gsearch1.best_score_ ,grid_test_predictions , grid_test_predictions_prob,test_accuracy_score "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_fit(alg_provided,alg,initialized_classifier,classifer_parameter,split_train_test,X,y,X_train,X_test,y_train,y_test,random_state,scoring,cv,cv_folds,printFeatureImportance, top_feat , gridsearch_usage , grid_serach_param , n_jobs , verbose):\n",
    "    import matplotlib.pylab as plt\n",
    "    %matplotlib inline\n",
    "    from matplotlib.pylab import rcParams\n",
    "    rcParams['figure.figsize'] = 12, 4\n",
    "    \n",
    "    ## Divide the provided data into train and test\n",
    "    if split_train_test == True:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "    print('X_train count : {} , X_test : {} '.format(len(X_train),len(X_test)))\n",
    "    \n",
    "    \n",
    "    #################################################\n",
    "    ## defining inputs\n",
    "    if alg_provided == True:\n",
    "        \n",
    "        model_dict = {'chosen Alg' : (alg,classifer_parameter,grid_serach_param)}\n",
    "\n",
    "    else:\n",
    "        model_dict = Parameter_initialization()\n",
    "        \n",
    "    \n",
    "    Output = {}\n",
    "    Accuracy = {}\n",
    "    Model = {}\n",
    "    \n",
    "    GS_Output = {}\n",
    "    GS_Accuracy = {}\n",
    "    GS_Model = {}\n",
    "    \n",
    "    for i in model_dict:\n",
    "        print('Algorithim {}'.format(i))\n",
    "        \n",
    "        if alg_provided == True:\n",
    "            if initialized_classifier == True:\n",
    "                if i == 'SVM':\n",
    "                    classifier = model_dict[i][0](**model_dict[i][1], probability = True).fit(X_train,y_train)\n",
    "                else:   \n",
    "                    classifier = model_dict[i][0](**model_dict[i][1]).fit(X_train,y_train)\n",
    "\n",
    "        else:\n",
    "            if i == 'SVM':\n",
    "                classifier = model_dict[i][0](probability = True).fit(X_train,y_train)\n",
    "            else:    \n",
    "                classifier = model_dict[i][0]().fit(X_train,y_train)\n",
    "        \n",
    "        ## Score test set\n",
    "        dtest_predictions,dtest_predprob,dtest_score = predict_values (classifier,X_test,y_test,scoring)\n",
    "        \n",
    "        ## Cross validation\n",
    "        if cv == True:\n",
    "            print('Cross Validation')\n",
    "            Cross_validation_results = cross_val_score(classifier, X, y, cv=cv_folds, scoring = scoring)\n",
    "            Output[i] = (dtest_predictions,dtest_predprob,dtest_score,classifier,np.mean(Cross_validation_results))\n",
    "        \n",
    "        ## collect default results in an output format\n",
    "        print('{} for Algorithim {} is {}'.format(scoring,i,dtest_score))\n",
    "        Output[i] = (dtest_predictions,dtest_predprob,dtest_score,classifier)\n",
    "        Accuracy[i] = dtest_score\n",
    "        Model[i] = classifier\n",
    "        \n",
    "        ##GridSearch\n",
    "        if gridsearch_usage == True:\n",
    "            print('Grid search')\n",
    "            gsearch1 , gsearch1_best_estimator , gsearch1_best_params , gsearch1_best_score , grid_test_predictions, grid_test_predictions_prob , acc_score = genral_purpose_gridsearch(i,model_dict[i][0],model_dict[i][2],random_state,n_jobs,cv_folds,verbose,X_train,y_train,scoring, X_test,y_test)\n",
    "        \n",
    "            gs_classifier = gsearch1_best_estimator\n",
    "    \n",
    "            GS_Output[i] = (gsearch1,gsearch1_best_estimator,acc_score)\n",
    "            GS_Accuracy[i] = acc_score\n",
    "            GS_Model[i] = gsearch1_best_estimator\n",
    "    #######################################################################################################################\n",
    "\n",
    "    # Choosing Best of default based on accuracy\n",
    "    if scoring.find('loss') != -1:\n",
    "        max_value = np.min(list(Accuracy.values()))\n",
    "    else:\n",
    "        max_value = np.max(list(Accuracy.values()))\n",
    "    position = int(np.where(list(Accuracy.values()) == max_value)[0])\n",
    "    Max_Name = list(Accuracy.keys())[position]\n",
    "    best_default_classifier = Output[Max_Name][3]\n",
    "    print('Best Accuracy Performance Algorithim :{} with Accuracy {}'.format(Max_Name,max_value))\n",
    "    \n",
    "    ## Choosing Best of grid serach based on accuracy\n",
    "    if gridsearch_usage == True:\n",
    "        ##Here I check whether the metric is \"loss\" : smaller is better , \"performance\" : greater is better\n",
    "        if scoring.find('loss') != -1:\n",
    "            max_value = np.min(list(GS_Accuracy.values()))\n",
    "        else:\n",
    "            max_value = np.max(list(GS_Accuracy.values()))\n",
    "            \n",
    "        position = int(np.where(list(GS_Accuracy.values()) == max_value)[0])\n",
    "        Max_Name = list(GS_Accuracy.keys())[position]\n",
    "        best_gs_classifier = GS_Output[Max_Name][1]\n",
    "        print('Best Accuracy Performance Algorithim :{} with Accuracy {}'.format(Max_Name,max_value))\n",
    "        return Output,Accuracy,Model,GS_Output,GS_Accuracy,GS_Model,best_default_classifier,best_gs_classifier\n",
    "    else:\n",
    "        return Output,Accuracy,Model,GS_Output,GS_Accuracy,GS_Model,best_default_classifier\n",
    "#######################################################################################################################\n",
    "    ##After foor loop we chose the best and print the features and other graphs\n",
    "    \n",
    "    ## Choosing Best of default\n",
    "#     max_value = np.max(list(Accuracy.values()))\n",
    "#     position = int(np.where(list(Accuracy.values()) == max_value)[0])\n",
    "#     Max_Name = list(Accuracy.keys())[position]\n",
    "#     best_default_classifier = Output[Max_Name][3]\n",
    "#     print('Best Accuracy Performance Algorithim :{} with Accuracy {}'.format(Max_Name,max_value))\n",
    "\n",
    "                     \n",
    "#     ## Choosing Best\n",
    "#     max_value = np.max(list(GS_Accuracy.values()))\n",
    "#     position = int(np.where(list(GS_Accuracy.values()) == max_value)[0])\n",
    "#     Max_Name = list(GS_Accuracy.keys())[position]\n",
    "#     best_gs_classifier = GS_Output[Max_Name][1]\n",
    "#     print('Best Accuracy Performance Algorithim :{} with Accuracy {}'.format(Max_Name,Max))\n",
    "\n",
    "#     ##Feature importance\n",
    "#     if printFeatureImportance == True:\n",
    "#         if alg_provided == False:\n",
    "#             feature_importance_plot (printFeatureImportance,alg_provided,best_default_classifier,Model,top_feat)\n",
    "\n",
    "#         else:\n",
    "#             feature_importance_plot (printFeatureImportance,alg_provided,classifier,0,top_feat)\n",
    "                     \n",
    "#     plot_learning_curve(50,0.2,0,gsearch1_best_estimator,X,y,'neg_log_loss',-1,(7,5))\n",
    "#     Plot_confusion_matrix(y_test,gsearch1_best_estimator,X_test,plt.cm.Blues)\n",
    "#     if model == 'SVM' and Model_output['SVM'].kernel != 'linear':\n",
    "#         print('SVM kernal is not Linear')\n",
    "#     else:\n",
    "#         feature_importance_plot (True,True,gsearch1_best_estimator,0,20)\n",
    "       \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(n_splits,test_size,random_state,estimator,estimator_name,X,y,scoring_metric,n_jobs,figsize):\n",
    "    cv = ShuffleSplit(n_splits=n_splits, test_size=test_size, random_state=random_state)\n",
    "    train_sizes, train_scores, test_scores = learning_curve(estimator,X,y,cv=cv,scoring = scoring_metric ,n_jobs = n_jobs)\n",
    "    train_scores_mean = train_scores.mean(axis=1)\n",
    "    test_scores_mean = test_scores.mean(axis=1)\n",
    "    \n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.title('learning_curve')\n",
    "    plt.ylabel(scoring_metric)\n",
    "    plt.xlabel('training data size')\n",
    "    plt.plot(train_sizes,train_scores_mean,'o-', color=\"r\",label=\"Training score\")\n",
    "    plt.plot(train_sizes,test_scores_mean,'o-', color=\"g\",label=\"Testing score\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Plot_confusion_matrix(y_test,estimator,X_test,cmap):\n",
    "    cm = confusion_matrix(y_test, estimator.predict(X_test))\n",
    "    classes = estimator.classes_\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.colorbar()\n",
    "    plt.title('Confusion Matrix')\n",
    "    tickmarks = np.arange(len(classes))\n",
    "    plt.xticks(tickmarks, classes, rotation=45)\n",
    "    plt.yticks(tickmarks,classes)\n",
    "\n",
    "\n",
    "    fmt = 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),horizontalalignment=\"center\",\n",
    "                color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_importance_plot (printFeatureImportance,alg_provided,classifier,Classifier_dict,top_feat,X):\n",
    "    ##Feature importance\n",
    "    if printFeatureImportance == True:\n",
    "        if alg_provided == False:\n",
    "            plt.subplot( len(Classifier_dict))\n",
    "            for model in Classifier_dict:\n",
    "                ##(gsearch1,gsearch1_best_estimator,acc_score)\n",
    "                \n",
    "                feat_imp = pd.Series(Classifier_dict[model].feature_importances_, X.columns).sort_values(ascending=False)\n",
    "                Top20 = feat_imp.head(top_feat)\n",
    "\n",
    "                Top20.plot(kind='bar', title='Feature Importances')\n",
    "                plt.ylabel('Feature Importance Score')\n",
    "                plt.show()\n",
    "\n",
    "        else:\n",
    "            feat_imp = pd.Series(classifier.feature_importances_, X.columns).sort_values(ascending=False)\n",
    "            Top20 = feat_imp.head(top_feat)\n",
    "\n",
    "            Top20.plot(kind='bar', title='Feature Importances')\n",
    "            plt.ylabel('Feature Importance Score')\n",
    "            plt.show()\n",
    "        return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
